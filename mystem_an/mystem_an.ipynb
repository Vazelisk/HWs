{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Арцишевский Антон, 183 группа, дз 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import json\n",
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Задание: Обработать книгу с помощью mystem:**\n",
    "##### **Просто читаю книгу, парсю майстемом и записываю как json в отдельный файл**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Затраченное время: 154.211284160614\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "m = Mystem()\n",
    "with open('feelpower_Asimov.txt', encoding='utf-8') as text:\n",
    "    text = text.read()\n",
    "lemmas = m.analyze(text)\n",
    "with open('mystem_Asimov.json', 'w', encoding='utf-8') as f:\n",
    "    json_mystem_Asimov = json.dump(lemmas, f, ensure_ascii=False)\n",
    "end = time.time()\n",
    "print('Затраченное время:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Задание: Обработать книгу через pymorphy**\n",
    "##### **Чищу текст, читаю разбор каждого слова, из которого беру нужные показатели. Запихиваю это в виде словаря в заранее созданный список и записываю его в качестве json строки в файл**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Затраченное время: 0.9683258533477783\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "words = [w.lower() for w in word_tokenize(text) if w.isalpha()]\n",
    "morph = MorphAnalyzer()\n",
    "gram_razbor = []\n",
    "for word in words:\n",
    "    morph_asimov = morph.parse(word)\n",
    "    wordform = morph_asimov[0].word\n",
    "    lexemma = morph_asimov[0].normal_form\n",
    "    tags = morph_asimov[0].tag.POS\n",
    "    simple_list = [{'СЛОВО': wordform, 'ЛЕКСЕМА': lexemma, 'ЧАСТЬ РЕЧИ': tags}]\n",
    "    gram_razbor.extend(simple_list)\n",
    "with open('pymorphy_Asimov.json', 'w', encoding='utf-8') as f:\n",
    "    json_pymorpy_Asimov = json.dump(gram_razbor, f, ensure_ascii=False)\n",
    "end = time.time()\n",
    "print('Затраченное время:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Задание: Какую долю слов составляет каждая часть речи?**\n",
    "##### **Беру разбор слова. Когда программа дойдёд до части речи, то я беру значение части речи и добавляю его в заранее созданный список. Получается список с перечеслением всех частей речи всех слов в тексте. Я считаю их с помощью Counter. Затем я считаю частотность для каждой части речи**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Частота частей речи:\n",
      "NOUN - 0.25626423690205014\n",
      "VERB - 0.14502657555049356\n",
      "INFN - 0.0425208807896735\n",
      "PREP - 0.0914958238420653\n",
      "ADJF - 0.10516324981017464\n",
      "PRTF - 0.008352315869400152\n",
      "NPRO - 0.08466211085801063\n",
      "CONJ - 0.10364464692482915\n",
      "ADVB - 0.05353075170842825\n",
      "ADJS - 0.011009870918754746\n",
      "PRCL - 0.06340167046317388\n",
      "None - 0.009870918754745633\n",
      "NUMR - 0.004555808656036446\n",
      "COMP - 0.007593014426727411\n",
      "PRED - 0.006454062262718299\n",
      "GRND - 0.0026575550493545936\n",
      "INTJ - 0.002277904328018223\n",
      "PRTS - 0.0015186028853454822\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_classes = []\n",
    "freq = []\n",
    "for i in gram_razbor:\n",
    "    for pare in i.items():\n",
    "        if 'ЧАСТЬ РЕЧИ' in pare:\n",
    "            word_class = pare[1]\n",
    "            word_classes.append(word_class)\n",
    "all_words = len(words)\n",
    "c = Counter(word_classes)\n",
    "values = c.items()\n",
    "print('Частота частей речи:')\n",
    "for x in values:\n",
    "    freq_class = x[1] / all_words\n",
    "    print(x[0], '-', freq_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Задание: Найдите топ-20 (по частотности) глаголов и наречий**\n",
    "##### **Перебираю разборы. Если в значениях разбора встречается глагол, то я создаю переменную лексемы глагола и пихунькаю её в заранее созданный словарь (либо обновляю его, если переменная уже лежит в словаре). Потом я сортирую топ-20 и красивенько его принчу. Повторяю операцию для наречий**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-20 глаголов и их кол-во вхождений:\n",
      "быть - 45\n",
      "мочь - 28\n",
      "сказать - 16\n",
      "говорить - 7\n",
      "возразить - 6\n",
      "видеть - 6\n",
      "произнести - 5\n",
      "хотеть - 5\n",
      "удаться - 5\n",
      "писать - 5\n",
      "продолжать - 5\n",
      "достать - 4\n",
      "делать - 4\n",
      "думать - 4\n",
      "получаться - 4\n",
      "подумать - 4\n",
      "суметь - 4\n",
      "увидеть - 3\n",
      "спросить - 3\n",
      "перемножить - 3\n",
      "\n",
      "Топ-20 наречий и их кол-во вхождений:\n",
      "сейчас - 7\n",
      "потом - 7\n",
      "всегда - 6\n",
      "теперь - 6\n",
      "потому - 4\n",
      "пока - 4\n",
      "очень - 4\n",
      "тоже - 3\n",
      "хорошо - 3\n",
      "снова - 3\n",
      "всего - 2\n",
      "несколько - 2\n",
      "совсем - 2\n",
      "там - 2\n",
      "слегка - 2\n",
      "нетерпеливо - 2\n",
      "много - 2\n",
      "почему - 2\n",
      "вдруг - 2\n",
      "трудно - 2\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "verbs = {}\n",
    "for i in gram_razbor:\n",
    "    if 'VERB' in i.values():\n",
    "        lexemma = i['ЛЕКСЕМА']\n",
    "        if lexemma in verbs:\n",
    "            verbs[lexemma] += 1\n",
    "        else:\n",
    "            verbs[lexemma] = 1\n",
    "sorted_verbs = sorted(verbs.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print('Топ-20 глаголов и их кол-во вхождений:')\n",
    "for x, y in sorted_verbs[0:20]:\n",
    "    print(x, '-', y)\n",
    "print('')\n",
    "\n",
    "advbs = {}\n",
    "for i in gram_razbor:\n",
    "    if 'ADVB' in i.values():\n",
    "        lexemma = i['ЛЕКСЕМА']\n",
    "        if lexemma in advbs:\n",
    "            advbs[lexemma] += 1\n",
    "        else:\n",
    "            advbs[lexemma] = 1\n",
    "sorted_advbs = sorted(advbs.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print('Топ-20 наречий и их кол-во вхождений:')\n",
    "for x, y in sorted_advbs[0:20]:\n",
    "    print(x, '-', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Задание: топ-25 биграмм и триграмм**\n",
    "##### **Удаляю пунктуацию, ищу биграммы по распличенному тексту, беру топ-25 и красиво печатаю. Повторяю операцию с триграммами**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самые частотные биграммы:\n",
      "Я + не - 7\n",
      "и + не - 5\n",
      "Генерал + Уэйдер - 4\n",
      "умножить + на - 4\n",
      "так + что - 4\n",
      "техник + Ауб - 4\n",
      "Он + был - 3\n",
      "возразил + Шуман - 3\n",
      "на + бумаге - 3\n",
      "как + вы - 3\n",
      "на + 3 - 3\n",
      "Это + значит - 3\n",
      "можно + будет - 3\n",
      "без + счетчика - 3\n",
      "что + это - 3\n",
      "и + в - 3\n",
      "Я + могу - 3\n",
      "говорит + что - 3\n",
      "может + воспроизвести - 3\n",
      "я + вижу - 3\n",
      "Программист + Шуман - 3\n",
      "вычислительных + машин - 3\n",
      "конце + концов - 3\n",
      "больше + не - 3\n",
      "счетных + машин - 2\n",
      "\n",
      "Самые частотные триграммы:\n",
      "достал + свой - 2\n",
      "7 + умножить - 2\n",
      "умножить + на - 2\n",
      "так + что - 2\n",
      "Шуман + говорит - 2\n",
      "теперь + я - 2\n",
      "В + конце - 2\n",
      "но + техник - 2\n",
      "Айзек + Азимов - 1\n",
      "Азимов + Чувство - 1\n",
      "Чувство + силы - 1\n",
      "силы + Джехан - 1\n",
      "Джехан + Шуман - 1\n",
      "Шуман + привык - 1\n",
      "привык + иметь - 1\n",
      "иметь + дело - 1\n",
      "дело + с - 1\n",
      "с + высокопоставленными - 1\n",
      "высокопоставленными + людьми - 1\n",
      "людьми + руководящими - 1\n",
      "руководящими + раздираемой - 1\n",
      "раздираемой + распрями - 1\n",
      "распрями + планетой - 1\n",
      "планетой + Он - 1\n",
      "Он + был - 1\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "b_result = \"\"\n",
    "for c in text:\n",
    "    if c not in string.punctuation:\n",
    "        b_result += c\n",
    "bigrm = list(nltk.bigrams(b_result.split()))\n",
    "c = Counter(bigrm)\n",
    "z = c.most_common(25)\n",
    "print('Самые частотные биграммы:')\n",
    "for key, value in z:\n",
    "    print(key[0], '+', key[1], '-', value)\n",
    "\n",
    "print('')\n",
    "\n",
    "t_result = \"\"\n",
    "for c in text:\n",
    "    if c not in string.punctuation:\n",
    "        t_result += c\n",
    "bigrm = list(nltk.trigrams(t_result.split()))\n",
    "c = Counter(bigrm)\n",
    "z = c.most_common(25)\n",
    "print('Самые частотные триграммы:')\n",
    "for key, value in z:\n",
    "    print(key[0], '+', key[1], '-', value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
